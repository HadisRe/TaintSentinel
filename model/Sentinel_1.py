 

import torch
import torch.nn as nn
import numpy as np
import json
from pathlib import Path
from torch_geometric.data import Data
import random
import warnings
warnings.filterwarnings('ignore')

 

class SmartContractDataset(torch.utils.data.Dataset):
    def __init__(self, base_path, batch_names=['batch1'], balanced=False, random_seed=42):
        """
        Dataset loader Ø¨Ø±Ø§ÛŒ Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯Ù‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯

        Args:
            base_path: Ù…Ø³ÛŒØ± Ù¾Ø§ÛŒÙ‡ Ø¯ÛŒØªØ§Ø³Øª
            batch_names: Ù„ÛŒØ³Øª Ù†Ø§Ù… batch Ù‡Ø§
            balanced: Ø¢ÛŒØ§ Ø¯ÛŒØªØ§Ø³Øª Ù…ØªÙˆØ§Ø²Ù† Ø´ÙˆØ¯
            random_seed: seed Ø¨Ø±Ø§ÛŒ ØªÚ©Ø±Ø§Ø±Ù¾Ø°ÛŒØ±ÛŒ
        """
        self.base_path = Path(base_path)
        self.batch_names = batch_names if isinstance(batch_names, list) else [batch_names]
        self.balanced = balanced

        # Ø®ÙˆØ§Ù†Ø¯Ù† Ù‡Ù…Ù‡ batch Ù‡Ø§
        all_entries = []
        total_safe = 0
        total_vuln = 0
        missing_files = 0

        for batch_name in self.batch_names:
            # Ù…Ø³ÛŒØ± ØµØ­ÛŒØ­: Ù‡Ø± batch Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ø®ÙˆØ¯Ø´
            batch_path = self.base_path / batch_name
            index_file = batch_path / "ml_dataset" / f"dataset_index_{batch_name}.json"

            if not index_file.exists():
                print(f"  {index_file} ÛŒØ§ÙØª Ù†Ø´Ø¯!")
                continue

            with open(index_file, 'r') as f:
                batch_data = json.load(f)

            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù†
            valid_entries = []
            batch_missing = 0

            for entry in batch_data['entries']:
                contract_id = entry['contract_id']

                # Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
                graph_path = batch_path / "ml_dataset" / "graphs" / f"{contract_id}.npz"
                path_path = batch_path / "ml_dataset" / "paths" / f"{contract_id}.npz"

                # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù‡Ø± Ø¯Ùˆ ÙØ§ÛŒÙ„
                if graph_path.exists() and path_path.exists():
                    entry['batch_name'] = batch_name
                    valid_entries.append(entry)
                else:
                    batch_missing += 1
                    if batch_missing <= 5:  # ÙÙ‚Ø· 5 Ù…ÙˆØ±Ø¯ Ø§ÙˆÙ„ Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¨Ø¯Ù‡
                        if not graph_path.exists():
                            print(f"    Missing graph file: {graph_path.name}")
                        if not path_path.exists():
                            print(f"    Missing path file: {path_path.name}")

            if batch_missing > 5:
                print(f"   ... and {batch_missing - 5} more missing files in {batch_name}")

            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† entries Ù…Ø¹ØªØ¨Ø±
            all_entries.extend(valid_entries)

            # Ø´Ù…Ø§Ø±Ø´
            batch_safe = len([e for e in valid_entries if e['label'] == 0])
            batch_vuln = len([e for e in valid_entries if e['label'] == 1])
            total_safe += batch_safe
            total_vuln += batch_vuln
            missing_files += batch_missing

            print(f"  {batch_name}: Safe={batch_safe}, Vulnerable={batch_vuln}, Missing={batch_missing}")

        # Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ safe Ùˆ vulnerable
        self.safe_entries = [e for e in all_entries if e['label'] == 0]
        self.vulnerable_entries = [e for e in all_entries if e['label'] == 1]

        print(f"\nğŸ“Š Ù…Ø¬Ù…ÙˆØ¹ Dataset:")
        print(f"   Safe contracts: {total_safe}")
        print(f"   Vulnerable contracts: {total_vuln}")
        if missing_files > 0:
            print(f"    Total missing files: {missing_files}")

        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¯Ø§Ø±ÛŒÙ…
        if len(all_entries) == 0:
            raise ValueError("No valid entries found! Please check your data files.")

        if balanced and len(self.safe_entries) == 0:
            raise ValueError("No safe contracts found for balanced dataset!")

        if balanced and len(self.vulnerable_entries) == 0:
            raise ValueError("No vulnerable contracts found for balanced dataset!")

        # Ù…ØªÙˆØ§Ø²Ù†â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²
        if balanced and len(self.safe_entries) > 0 and len(self.vulnerable_entries) > 0:
            random.seed(random_seed)
            min_size = min(len(self.safe_entries), len(self.vulnerable_entries))
            self.safe_entries = random.sample(self.safe_entries, min_size)
            self.entries = self.safe_entries + self.vulnerable_entries
            print(f"    Balanced to: {min_size} samples each")
        else:
            self.entries = all_entries

        # Shuffle
        random.seed(random_seed)
        random.shuffle(self.entries)

        print(f"\n Total valid samples in dataset: {len(self.entries)}")

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        entry = self.entries[idx]
        contract_id = entry['contract_id']
        batch_name = entry['batch_name']

        # Ù…Ø³ÛŒØ± ØµØ­ÛŒØ­ Ø¨Ø±Ø§ÛŒ Ù‡Ø± batch
        batch_path = self.base_path / batch_name

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø±Ø§Ù
        graph_path = batch_path / "ml_dataset" / "graphs" / f"{contract_id}.npz"
        try:
            graph_data = np.load(graph_path)
        except Exception as e:
            print(f"\n Error loading graph file: {graph_path}")
            print(f"   Error: {str(e)}")
            raise

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø³ÛŒØ±
        path_path = batch_path / "ml_dataset" / "paths" / f"{contract_id}.npz"
        try:
            path_data = np.load(path_path)
        except Exception as e:
            print(f"\n Error loading path file: {path_path}")
            print(f"   Error: {str(e)}")
            raise

        # Ø§ÛŒØ¬Ø§Ø¯ PyTorch Geometric Data object
        edge_index = torch.tensor(graph_data['edge_index'], dtype=torch.long)

        # ØªØ±Ú©ÛŒØ¨ node features
        node_features = torch.tensor(graph_data['node_features'], dtype=torch.float)

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† node types Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† one-hot
        num_nodes = int(graph_data['num_nodes'])
        node_types = graph_data['node_types']
        max_node_type = 10
        node_type_onehot = torch.zeros(num_nodes, max_node_type)

        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ node_types Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ø¬Ø§Ø² Ø§Ø³Øª
        valid_types = node_types < max_node_type
        node_type_onehot[valid_types, node_types[valid_types]] = 1

        # ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ node features
        x = torch.cat([node_features, node_type_onehot], dim=1)

        # Ø³Ø§Ø®Øª graph data
        graph = Data(
            x=x,
            edge_index=edge_index,
            num_nodes=num_nodes
        )

        # Path data - Ø¨Ø¯ÙˆÙ† Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªØ¹Ø¯Ø§Ø¯ Ù…Ø³ÛŒØ±
        max_seq_len = 20  # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ù‡Ø± Ù…Ø³ÛŒØ± (Ø§ÛŒÙ† Ø±Ø§ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ sequence)

        path_sequences = path_data['path_sequences']
        path_lengths = path_data['path_lengths']
        path_features = path_data['path_features']
        risk_levels = path_data['risk_levels']
        num_paths = int(path_data['num_paths'])

        # ÙÙ‚Ø· padding Ø¨Ø±Ø§ÛŒ Ø·ÙˆÙ„ sequence Ù‡Ø§ (Ù†Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø³ÛŒØ±Ù‡Ø§)
        if num_paths > 0:
            current_seq_len = path_sequences.shape[1]
            if current_seq_len < max_seq_len:
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† padding Ø¨Ù‡ Ø·ÙˆÙ„ sequences
                pad_width = ((0, 0), (0, max_seq_len - current_seq_len))
                path_sequences = np.pad(path_sequences, pad_width, mode='constant', constant_values=0)
            elif current_seq_len > max_seq_len:
                # Ø¨Ø±Ø´ Ø§Ú¯Ø± Ø®ÛŒÙ„ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨ÙˆØ¯
                path_sequences = path_sequences[:, :max_seq_len]
                # Ø·ÙˆÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ù‡Ù… Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                path_lengths = np.minimum(path_lengths, max_seq_len)
        else:
            # Ø§Ú¯Ø± Ø§ØµÙ„Ø§Ù‹ Ù…Ø³ÛŒØ±ÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ…ØŒ ÛŒÚ© Ù…Ø³ÛŒØ± Ø®Ø§Ù„ÛŒ Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒÙ…
            path_sequences = np.zeros((1, max_seq_len))
            path_lengths = np.zeros(1)
            path_features = np.zeros((1, 6))
            risk_levels = np.zeros(1)
            num_paths = 0  # Ø§ÛŒÙ† Ù…Ù‡Ù… Ø§Ø³Øª Ú©Ù‡ 0 Ø¨Ù…Ø§Ù†Ø¯

        paths = {
            'sequences': torch.tensor(path_sequences, dtype=torch.long),
            'lengths': torch.tensor(path_lengths, dtype=torch.long),
            'features': torch.tensor(path_features, dtype=torch.float),
            'risk_levels': torch.tensor(risk_levels, dtype=torch.long),
            'num_paths': num_paths
        }

        # Label
        label = torch.tensor(entry['label'], dtype=torch.long)

        return {
            'graph': graph,
            'paths': paths,
            'label': label,
            'contract_id': contract_id,
            'has_paths': entry['num_paths'] > 0
        }

 

def test_dataset():
    """ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ dataset Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú¯Ù…Ø´Ø¯Ù‡"""
    print("  Testing Dataset Loader (Fixed Version)...")
    print("="*60)

    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    base_path = r"C:\Users\Hadis\Documents\NewModel1"

    # ØªØ³Øª Ø¨Ø§ batch1
    print("\n1ï¸ Testing with batch1 only:")
    dataset = SmartContractDataset(
        base_path=base_path,
        batch_names=['batch1'],
        balanced=False
    )

    # Ù†Ù…Ø§ÛŒØ´ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡
    print(f"\n Ù†Ù…Ø§ÛŒØ´ 3 Ù†Ù…ÙˆÙ†Ù‡ Ø§ÙˆÙ„:")
    for i in range(min(3, len(dataset))):
        try:
            sample = dataset[i]
            print(f"\n   Sample {i+1}:")
            print(f"   - Contract ID: {sample['contract_id']}")
            print(f"   - Label: {'Vulnerable' if sample['label'] == 1 else 'Safe'}")
            print(f"   - Graph nodes: {sample['graph'].x.shape[0]}")
            print(f"   - Graph edges: {sample['graph'].edge_index.shape[1]}")
            print(f"   - Has paths: {sample['has_paths']}")
            print(f"   - Number of paths: {sample['paths']['num_paths']}")
            print(f"   - Path sequences shape: {sample['paths']['sequences'].shape}")
        except Exception as e:
            print(f"    Error loading sample {i+1}: {str(e)}")

    # ØªØ³Øª balanced dataset
    print("\n\n  Testing balanced dataset:")
    try:
        dataset_balanced = SmartContractDataset(
            base_path=base_path,
            batch_names=['batch1'],
            balanced=True
        )

        # Ø´Ù…Ø§Ø±Ø´ labels
        labels = [dataset_balanced[i]['label'].item() for i in range(len(dataset_balanced))]
        safe_count = labels.count(0)
        vuln_count = labels.count(1)
        print(f"\n   Balanced dataset: Safe={safe_count}, Vulnerable={vuln_count}")
    except Exception as e:
        print(f"    Error creating balanced dataset: {str(e)}")

    # ØªØ³Øª batch1 Ùˆ batch2
    print("\n\n Testing with both batches:")
    try:
        dataset_full = SmartContractDataset(
            base_path=base_path,
            batch_names=['batch1', 'batch2'],
            balanced=False
        )
        print("   Both batches loaded successfully!")

        # Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú¯Ù…Ø´Ø¯Ù‡
        print(f"\n   Total valid entries: {len(dataset_full)}")

    except Exception as e:
        print(f"    Error loading batches: {str(e)}")

    print("\n Dataset loader test completed!")


if __name__ == "__main__":
    test_dataset()
